{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the data.\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#reading in the data, this time in the form of paragraphs\n",
    "emma=gutenberg.paras('austen-emma.txt')\n",
    "#processing\n",
    "emma_paras=[]\n",
    "for paragraph in emma:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    emma_paras.append(' '.join(para))\n",
    "\n",
    "# Creating the tf-idf matrix.\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "emma_paras_tfidf=vectorizer.fit_transform(emma_paras)\n",
    "\n",
    "# Getting the word list.\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# Number of topics.\n",
    "ntopics=5\n",
    "\n",
    "# Linking words to topics\n",
    "def word_topic(tfidf,solution, wordlist):\n",
    "    \n",
    "    # Loading scores for each word on each topic/component.\n",
    "    words_by_topic=tfidf.T * solution\n",
    "\n",
    "    # Linking the loadings to the words in an easy-to-read way.\n",
    "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Extracts the top N words and their loadings for each topic.\n",
    "def top_words(components, n_top_words):\n",
    "    n_topics = range(components.shape[1])\n",
    "    index= np.repeat(n_topics, n_top_words, axis=0)\n",
    "    topwords=pd.Series(index=index)\n",
    "    for column in range(components.shape[1]):\n",
    "        # Sort the column so that highest loadings are at the top.\n",
    "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
    "        # Choose the N highest loadings.\n",
    "        chosen=sortedwords[:n_top_words]\n",
    "        # Combine loading and index into a string.\n",
    "        chosenlist=chosen.index +\" \"+round(chosen,2).map(str) \n",
    "        topwords.loc[column]=[x for x in chosenlist]\n",
    "    return(topwords)\n",
    "\n",
    "# Number of words to look at for each topic.\n",
    "n_top_words = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd= TruncatedSVD(ntopics)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "emma_paras_lsa = lsa.fit_transform(emma_paras_tfidf)\n",
    "\n",
    "components_lsa = word_topic(emma_paras_tfidf, emma_paras_lsa, terms)\n",
    "\n",
    "topwords=pd.DataFrame()\n",
    "topwords['LSA']=top_words(components_lsa, n_top_words)                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "lda = LDA(n_topics=ntopics, \n",
    "          doc_topic_prior=None, # Prior = 1/n_documents\n",
    "          topic_word_prior=1/ntopics,\n",
    "          learning_decay=0.7, # Convergence rate.\n",
    "          learning_offset=10.0, # Causes earlier iterations to have less influence on the learning\n",
    "          max_iter=10, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          evaluate_every=-1, # Do not evaluate perplexity, as it slows training time.\n",
    "          mean_change_tol=0.001, # Stop updating the document topic distribution in the E-step when mean change is < tol\n",
    "          max_doc_update_iter=100, # When to stop updating the document topic distribution in the E-step even if tol is not reached\n",
    "          n_jobs=-1, # Use all available CPUs to speed up processing time.\n",
    "          verbose=0, # amount of output to give while iterating\n",
    "          random_state=0\n",
    "         )\n",
    "\n",
    "emma_paras_lda = lda.fit_transform(emma_paras_tfidf) \n",
    "\n",
    "components_lda = word_topic(emma_paras_tfidf, emma_paras_lda, terms)\n",
    "\n",
    "topwords['LDA']=top_words(components_lda, n_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNMF\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(alpha=0.0, \n",
    "          init='nndsvdar', # how starting value are calculated\n",
    "          l1_ratio=0.0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)\n",
    "          max_iter=200, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          n_components=ntopics, \n",
    "          random_state=0, \n",
    "          solver='cd', # Use Coordinate Descent to solve\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol\n",
    "          verbose=0 # amount of output to give while iterating\n",
    "         )\n",
    "emma_paras_nmf = nmf.fit_transform(emma_paras_tfidf) \n",
    "\n",
    "components_nmf = word_topic(emma_paras_tfidf, emma_paras_nmf, terms)\n",
    "\n",
    "topwords['NNMF']=top_words(components_nmf, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "               LSA             NNMF\n",
      "0         oh 97.61         oh 30.12\n",
      "0         yes 4.82         yes 0.52\n",
      "0  difference 1.58       sorry 0.16\n",
      "0       thank 1.46  difference 0.15\n",
      "0        just 1.44        just 0.12\n",
      "0     harriet 1.33        papa 0.12\n",
      "0       sorry 1.28        week 0.11\n",
      "0       short 1.17       short 0.11\n",
      "0        papa 1.15    recollect 0.1\n",
      "0       cried 1.02      letter 0.09\n",
      "Topic 1:\n",
      "             LSA            NNMF\n",
      "1     emma 58.16         mr 7.29\n",
      "1       mr 58.09        mrs 4.26\n",
      "1     said 50.51      elton 3.41\n",
      "1      mrs 47.08  knightley 3.24\n",
      "1     miss 37.47     weston 3.15\n",
      "1  harriet 35.14       miss 2.99\n",
      "1   weston 32.29  woodhouse 2.09\n",
      "1    elton 27.75       said 1.88\n",
      "1      did 27.59    fairfax 1.84\n",
      "1    think 27.23       emma 1.47\n",
      "Topic 2:\n",
      "             LSA           NNMF\n",
      "2       ah 31.15       ah 12.89\n",
      "2      sure 3.67      sure 0.35\n",
      "2       say 1.08   believe 0.18\n",
      "2   believe 0.82       say 0.18\n",
      "2      sorry 0.8     shake 0.09\n",
      "2      shake 0.5     hands 0.09\n",
      "2      hands 0.5     sorry 0.09\n",
      "2  grievous 0.44  grievous 0.08\n",
      "2      poor 0.43      come 0.07\n",
      "2     spring 0.4    taylor 0.07\n",
      "Topic 3:\n",
      "             LSA           NNMF\n",
      "3  chapter 33.91  chapter 10.46\n",
      "3         ii 3.2       iii 0.68\n",
      "3        iii 3.2        ii 0.68\n",
      "3     xviii 2.52       vii 0.66\n",
      "3        xv 2.52        xv 0.66\n",
      "3        ix 2.52       xvi 0.66\n",
      "3       xiv 2.52      xvii 0.66\n",
      "3        vi 2.52     xviii 0.66\n",
      "3       xvi 2.52       xii 0.66\n",
      "3        iv 2.52        xi 0.66\n",
      "Topic 4:\n",
      "               LSA          NNMF\n",
      "4         mr 38.15     emma 9.27\n",
      "4  knightley 18.91     said 5.84\n",
      "4       elton 15.3     dear 3.52\n",
      "4      weston 9.27  harriet 2.89\n",
      "4         mrs 6.62      yes 2.29\n",
      "4        john 4.53    think 1.74\n",
      "4      martin 3.22       mr 1.69\n",
      "4   woodhouse 2.76      say 1.52\n",
      "4        idea 1.98    thing 1.32\n",
      "4   churchill 1.72     sure 1.27\n"
     ]
    }
   ],
   "source": [
    "for topic in range(ntopics):\n",
    "    print('Topic {}:'.format(topic))\n",
    "    print(topwords.loc[topic])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
